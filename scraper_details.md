# Leaderboard Scraper - Architecture Documentation

> **Purpose**: This document is the authoritative reference for the scraper architecture. AI assistants MUST read and follow this document when working on scraper code. It prevents architectural drift, code mixing, and ensures consistent patterns.

---

# ğŸ¤– AI CONTINUITY NOTES (Read First!)

**Current Version**: v7.36 (February 4, 2026)

## Recent Session Summary (v7.36)

### What Was Fixed
1. **paxgambles.com - Bogus "Paxgambles" Entry Being Extracted**
   - Root cause: Site branding text "Paxgambles" was being parsed as a leaderboard entry with rank #10
   - The `KNOWN_WEBSITE_NAMES` list in entry-validation.js didn't include reward site names
   - **Fix**: Added common reward site names to `KNOWN_WEBSITE_NAMES`:
     - paxgambles, wrewards, devlrewards, goatgambles, codeshury
     - betjuicy, birb, muta, elliotrewards, crunchyrewards, augustrewards
     - scrapesgambles, jonkenn, vinnyvh, tanskidegen, yeeterboards
   - Result: paxgambles.com/cases now correctly shows 5 entries (was 6 with bogus "Paxgambles")

2. **[hidden] Placeholder Entries with No Data**
   - Root cause: `[hidden]` placeholder entries with wager=0 AND prize=0 were counted as real entries
   - These are created when extraction finds a position but no username - indicates parsing error
   - **Fix**: Added filter in `validateAndCleanEntries()` to reject `[hidden]` entries with no data
   - Result: Cleaner entry counts without bogus placeholders

3. **acebet Leaderboard Not Being Saved (Minimum Entry Threshold)**
   - Root cause: Orchestrator required minimum 3 entries (`hasValidData = extraction.entries.length >= 3`)
   - acebet only had 2 real users, so it was flagged as "data quality poor" and not saved
   - **Fix**: Lowered minimum threshold from 3 to 2 entries
   - Some leaderboards legitimately have only 2-3 participants early in a competition
   - Result: acebet now correctly shows 2 entries

### Files Modified in v7.36
- `/Users/cj/Desktop/ARGUS/lbscraper/shared/entry-validation.js`:
  - Added reward site names to KNOWN_WEBSITE_NAMES (lines 350-352)
  - Added [hidden] placeholder filter in validateAndCleanEntries() (lines 451-456)
- `/Users/cj/Desktop/ARGUS/lbscraper/orchestrators/scrape-orchestrator.js`:
  - Changed minimum entry threshold from 3 to 2 (line 541)

### Regression Test Results (4 Reference Sites)
| Site | Leaderboards | Entries | Status |
|------|--------------|---------|--------|
| paxgambles.com | 3 (diceblox, cases, acebet) | 10, 5, 2 | âœ… Fixed |
| devlrewards.com | 3 (rainbet, rain, packdraw) | 50, 50, 50 | âœ… Working |
| goatgambles.com | 6 LBs | 3 each | âœ… Working |
| wrewards.com | 4 (hunt, gamdom, packdraw, lootbox) | 50, 30, 25, 15 | âœ… Working |

---

## Previous Session Summary (v7.35)

### What Was Fixed
1. **codeshury.com - Bogus Rank 20/40 Entries with [hidden] Usernames**
   - Root cause: The list parser's `rankPattern` matched bare numbers like `40` and `20` as ranks
   - These numbers were actually PRIZE values in table cells ($40 prize, $20 prize)
   - Pattern `/^(?:\*\*)?#?(?:\*\*)?\s*(\d+)(?:\*\*)?\.?$/` had `#?` making hash optional
   - **Fix**: Changed `rankPattern` to REQUIRE `#` prefix: `/^(?:\*\*)?#(?:\*\*)?\s*(\d+)(?:\*\*)?$/`
   - Added separate `rankPatternDot` for explicit period suffix (e.g., "4.")
   - Result: codeshury.com/csgold now correctly shows 10 entries (was 12 with bogus rank 20/40)

### Files Modified in v7.35
- `/Users/cj/Desktop/ARGUS/lbscraper/shared/extraction.js`:
  - Changed `rankPattern` to require `#` prefix (line 1267)
  - Added `rankPatternDot` for period suffix ranks (line 1270)
  - Updated matching logic to use all three patterns (line 1286)

---

## Previous Session Summary (v7.34)

### What Was Fixed
1. **codeshury.com and muta.bet - False "clash" Leaderboard Detection**
   - Root cause: The crawler's `hasEvidence` check was matching CSS class names containing keywords
   - Example: `<img class="game-image clash-image">` triggered a match for "clash" even though it was just a styling class applied to ALL game images
   - The images had src URLs like `upgrader.png` and `csgold.png` but the class contained "clash"
   - **Fix**: Changed image evidence check to ONLY look at `src` and `alt` attributes, NOT `className`
   - Added stricter src matching: keyword must appear as path segment (e.g., `/shuffle.png` not just contains)
   - Also cleaned up stale site profiles that had recorded the invalid "clash" leaderboard
   - Result: codeshury.com now correctly shows 2 LBs (upgrader, csgold), muta.bet shows 2 LBs (shuffle, packdraw)

### Files Modified in v7.34
- `/Users/cj/Desktop/ARGUS/lbscraper/core/leaderboard-crawler.js`:
  - Changed image evidence check to only use src and alt (lines 140-156)
  - Added stricter path segment matching for src URLs
- `/Users/cj/Desktop/ARGUS/lbscraper/data/site-profiles/codeshury.com.json`:
  - Removed invalid "clash" leaderboard entry
- `/Users/cj/Desktop/ARGUS/lbscraper/data/site-profiles/muta.bet.json`:
  - Removed invalid "clash" leaderboard entry

---

## Previous Session Summary (v7.33)

### What Was Fixed
1. **muta.bet - Duplicate Entries with Swapped Wager/Prize Values**
   - Root cause 1: Podium parser assigned ranks by parsing order (1st found = rank 1), not by explicit rank
   - Sites like muta.bet have rank 2 appearing BEFORE rank 1 in the markdown (visual layout)
   - **Fix 1**: Added detection for explicit rank pattern `/^(\d+)\.$/` (e.g., "2.", "1.")
   - Now correctly parses ranks from the markdown instead of assuming order

   - Root cause 2: List parser was picking up podium entries with "Wagered:" label
   - These entries use "Prize then Wager" column order in the table section
   - But podium entries have "Wager then Prize" format - causing swapped values
   - **Fix 2**: Added check in list parser to skip entries that have "Wagered:" label in lookahead
   - Entries with "Wagered:" label are handled by podium parser only
   - Result: muta.bet reduced from 23 to 20 entries (no duplicates), correct wager/prize values

### Files Modified in v7.33
- `/Users/cj/Desktop/ARGUS/lbscraper/shared/extraction.js`:
  - Added explicit rank detection `explicitRankPattern = /^(\d+)\.$/` in podium parser (line 1170)
  - Added `isPodiumEntry` check in list parser to skip "Wagered:" label entries (lines 1300-1309)

---

## Previous Session Summary (v7.32)

### What Was Fixed
1. **API Extraction - Split Format Support (dustintfp.com and similar)**
   - Added handling for `top_three` + `rest_of_users` combined format in API extraction
   - This format is used by BetSync-powered sites like dustintfp.com
   - The API extractor now merges these arrays and assigns proper ranks (1-3 for top_three, 4+ for rest)
   - **Fix**: Added detection in `strategies/api-extraction.js` for split format responses

2. **dustintfp.com - Still Shows Same Data for All Tabs (Known Issue)**
   - Root cause: Site uses client-side state switching - NO API calls when clicking tabs
   - All platform data is fetched on initial page load (`/api/r/leaderboard?platform=X`)
   - When clicking tabs, no new API requests are made
   - **Workaround needed**: Direct API requests for each platform pattern detected on initial load
   - **Status**: Partial fix - API extraction works for first tab, needs direct API request feature

### Files Modified in v7.32
- `/Users/cj/Desktop/ARGUS/lbscraper/strategies/api-extraction.js`:
  - Added `top_three` + `rest_of_users` split format detection (lines 111-125)
  - Added logging for split format detection

---

## Previous Session Summary (v7.31)

### What Was Fixed
1. **codeshury.com - Missing Ranks 4-5 in upgrader/clash**
   - Root cause 1: Username "CODE Leaderboard" was being rejected by `looksLikeWebsiteName()` in entry-validation.js
   - The function used `.includes()` to check if username contains any known website name
   - "CODE Leaderboard" contains "leaderboard" which was in KNOWN_WEBSITE_NAMES list
   - **Fix 1**: Changed to exact match only with `.includes(textLower)` instead of substring matching

   - Root cause 2: Username "CODE Leaderboard" was also flagged as UI text by `isUIText()` in utils.js
   - The function marked ANY 2-4 word phrase as UI text if ALL words were in the blacklist
   - "code" and "leaderboard" are both in UI_SINGLE_WORDS
   - **Fix 2**: Changed to require 3+ words (not 2) before marking as UI text
   - Two-word phrases like "CODE Leaderboard" are now allowed as valid usernames

   - Root cause 3: Rank 5 had no username - just an avatar image with no text after it
   - The imgUsernamePattern `^!\[.*?\]\([^)]+\)(.+)$` requires text after the image
   - **Fix 3**: Added `[hidden]` placeholder for entries with rank and wager but no username
   - This preserves entries where the site only shows an avatar (site-side data issue)

   - Root cause 4: Skip pattern `/^Leaderboard/i` in extraction.js skipPatterns was too broad
   - **Fix 4**: Changed to exact match `/^Leaderboard$/i` to not skip usernames like "CODE Leaderboard"
   - Result: codeshury.com upgrader/clash now extract 10 entries (was 8 before)

### Files Modified in v7.31
- `/Users/cj/Desktop/ARGUS/lbscraper/shared/entry-validation.js`:
  - Changed `looksLikeWebsiteName()` to use exact match instead of substring match (line 362)

- `/Users/cj/Desktop/ARGUS/lbscraper/shared/utils.js`:
  - Changed `isUIText()` to require 3+ words (not 2) for multi-word blacklist matching (line 239)

- `/Users/cj/Desktop/ARGUS/lbscraper/shared/extraction.js`:
  - Added `[hidden]` placeholder for entries with rank/wager but no username (lines 1457-1460)
  - Changed `/^Leaderboard/i` skip pattern to exact match `/^Leaderboard$/i` (line 1515)
  - Also made `/^Community$/i`, `/^Referral$/i`, `/^Social$/i` exact matches to avoid false positives

---

## Previous Session Summary (v7.30)

### What Was Fixed
1. **birb.bet Header Parser - Position Badge Numbers Misread as Wagers**
   - Root cause: Header parser (`parseMarkdownHeaderEntries`) was skipping image lines containing amounts like `![coin](...) 7,770.83`
   - It had a skip pattern `/^!\[/` that skipped ALL image lines, missing the wager/prize amounts
   - Additionally, bare position badge numbers (1, 2, 3) on their own lines were being picked up as wagers
   - **Fix Part 1**: Changed skip logic to only skip "image-only" lines (avatars), not images with amounts
   - **Fix Part 2**: Added position badge detection - look for bare 1-10 BEFORE `### username` header to use as rank
   - **Fix Part 3**: Added `stripImages()` helper to extract amounts from lines like `![coin](...) 7,770.83`
   - **Fix Part 4**: Skip bare small integers (1-10) without currency symbols as likely position badges
   - Result: birb.bet csgobig reduced from 14 entries (with duplicates) to 10 clean entries with correct wagers

### Files Modified in v7.30
- `/Users/cj/Desktop/ARGUS/lbscraper/shared/extraction.js`:
  - Added `stripImages()` helper in `parseMarkdownHeaderEntries` (line 1492)
  - Added `isImageOnlyLine()` function to detect avatar-only images (lines 1507-1513)
  - Changed skip logic to allow images with amounts (line 1600)
  - Added position badge detection looking 1-3 lines before header (lines 1558-1567)
  - Updated amount parsing to strip images and detect coin icons (lines 1624-1645)

---

## Previous Session Summary (v7.29)

### What Was Fixed
1. **DOM Entries Being Filtered Incorrectly** - Sites like scrapesgambles.com showing only 3 entries when 10 exist
   - Root cause: Fusion layer used markdown's max rank (3) as ceiling to filter DOM entries
   - DOM found 11 entries correctly, but entries 4-11 were filtered as "beyond max rank 3"
   - **Fix**: Added logic in `data-fusion.js` to trust high-confidence DOM (>=85%) if it finds MORE entries than markdown
   - Now: scrapesgambles.com extracts 10 entries instead of 3

2. **Prizes Not Extracted for Ranks 4-10** - Sites like tanskidegen.com showing $0 prizes after top 3
   - Root cause: List parser in `extraction.js` only handled `Prize: VALUE` (colon + value on same line)
   - But many sites have "Prize" on one line, amount on next: `Prize\n![Currency]200`
   - **Fix**: Added handling for "Prize" and "Wagered" labels without colon, looking for amount on next line
   - Now: All 10 entries have correct prizes extracted

### Files Modified in v7.29
- `/Users/cj/Desktop/ARGUS/lbscraper/core/data-fusion.js` - Trust high-confidence DOM when it finds more entries than markdown (lines 330-373)
- `/Users/cj/Desktop/ARGUS/lbscraper/shared/extraction.js` - Handle "Prize" and "Wagered" labels when amount is on next line (lines 1329-1358)

---

## Previous Session Summary (v7.28)

### What Was Fixed
1. **Vision Teaching - callClaude API Parameter Mismatch** - Vision teaching was failing due to incorrect API call format
   - Root cause: `visionAnalyze` function in Vision teaching script called `callClaude(STRUCTURE_PROMPT, imageContent)` but `callClaude` expects `{ systemPrompt, userMessage, basePath, imageBase64, domain }`
   - Also expected `response.text` but `callClaude()` returns `{ success, content, usage, error }`
   - **Fix**: Rewrote the Vision API call to use correct parameter format

2. **Single-Leaderboard Sites (scrapesgambles.com 404 Fix)** - Sites with only one leaderboard were failing with 404 errors
   - Root cause: Crawler detected leaderboard name (e.g., "stake") from page heading like "STAKE LEADERBOARD"
   - Detection method was `detected-name` with no click coordinates
   - Orchestrator tried URL navigation to `/leaderboard/stake` which returned 404
   - Scraper then SKIPPED the leaderboard entirely, missing the data
   - **Fix**: When URL navigation fails (404), go back to original page and extract from there
   - The leaderboard data was already visible on the current page - no navigation needed
   - Changed behavior: Instead of `continue` (skip), now proceeds with extraction

### Scripts Created in v7.28
- `/Users/cj/Desktop/ARGUS/lbscraper/run-vision-teaching-50.js` - Run Vision teaching on 50 unique sites from websites.txt
- `/Users/cj/Desktop/ARGUS/lbscraper/test-learned-configs.js` - Test learned configs with normal scraper (20 sites)
- `/Users/cj/Desktop/ARGUS/lbscraper/test-with-full-report.js` - Generate HTML report showing all entries per leaderboard
- `/Users/cj/Desktop/ARGUS/lbscraper/run-full-scrape-report.js` - Full scrape using REAL orchestrator with HTML report of all entries

### Files Modified in v7.28
- `/Users/cj/Desktop/ARGUS/lbscraper/orchestrators/scrape-orchestrator.js` - Fixed `detected-name` handling to extract from current page when URL navigation fails (lines 369-427)

### Vision Teaching Results (50 sites)
- 24 matches (48%), 17 mismatches (34%), 9 failed (18%)
- 41 configs saved to site-profiles
- Total cost: $0.36

### Testing Results
- **test-learned-configs.js** (20 sites): 70% success rate (14/20)
- **run-full-scrape-report.js** (30 sites): 29/30 successful, 79 leaderboards, 1,018 entries
- **scrapesgambles.com**: Now extracts 3 entries from stake leaderboard (was failing with 404)

### Full Scrape Report (30 sites, Feb 3 2026)
**Overall Stats:**
- Sites: 29/30 successful (96.7%)
- Leaderboards: 79 total
- Entries: 1,018 total
- Report: `/Users/cj/Desktop/ARGUS/lbscraper/debug/full-scrape-report/full-report.html`

**Failed Sites:**
| Site | Error |
|------|-------|
| crunchyrewards.com | No entries for stake; Click failed for bsite: undefined |

**Warnings Detected:**
- **Wager Order Violations** (lower-ranked users have higher wagers): csgowin, csgobig, shuffle, acebet, hypedrop, rain
- **Absurd Prize/Wager Ratios** (prize > wager): raingg, clash, gamba, csbattle, packdraw, csgogem
- **Duplicate Wagers**: chicken, csgowin, csgogem
- **Click Errors**: gamba, csgold, csgoluck, upgrader, csbattle (page navigated during click)

---

## Previous Session Summary (v7.27)

### What Was Fixed
1. **Vision Teaching System - Strategy Validation** - Vision's expected rank #1 is now used to validate extraction strategies
   - Root cause: When Vision taught the scraper about site structure (podiumLayout, prizeBeforeWager), the fusion layer still selected strategies by confidence alone
   - This meant DOM extraction (90 confidence) could beat markdown (87 confidence) even when markdown had the correct data matching Vision's expected rank #1
   - **Fix**: Added Vision validation in `data-fusion.js` that compares each strategy's extracted rank #1 against Vision's expected values
   - Strategies that match Vision get +15 confidence boost
   - Strategies that don't match Vision get -20 confidence penalty
   - This ensures the fusion layer prefers strategies that extract the correct leaderboard

2. **Vision Expected Rank #1 Passthrough** - Vision's learned rank #1 data now flows through the entire pipeline
   - Added `expectedRank1: { username, wager, prize }` to extraction config
   - Modified `test-vision-then-scrape.js` to pass Vision's rank #1 data to the scraper
   - Modified `data-extractor.js` to accept and forward `expectedRank1` config
   - Modified `data-fusion.js` to use `expectedRank1` for strategy validation

### Files Modified in v7.27
- `/Users/cj/Desktop/ARGUS/lbscraper/test-vision-then-scrape.js` - Pass Vision's rank #1 data to scraper config
- `/Users/cj/Desktop/ARGUS/lbscraper/core/data-extractor.js` - Accept and forward expectedRank1 config
- `/Users/cj/Desktop/ARGUS/lbscraper/core/data-fusion.js` - Vision validation logic with confidence adjustment

### How Vision Teaching Works
1. Vision analyzes screenshot and learns site structure (column order, podium layout, rank #1 data)
2. Scraper runs extraction strategies (API, markdown, DOM, etc.)
3. **NEW**: Fusion layer validates each strategy's rank #1 against Vision's expected data
4. Strategies matching Vision get confidence boost, mismatches get penalty
5. Highest-confidence strategy after validation is selected as primary

### Reference Sites (4 sites) - All Passing âœ…
- **paxgambles.com**: 3 LBs, 10 entries each
- **devlrewards.com**: 3 LBs, 50+ entries
- **goatgambles.com**: 6 LBs, 100 entries total
- **wrewards.com**: 4 LBs, 117 entries total

---

## Previous Session Summary (v7.26)

### What Was Fixed
1. **devlrewards.com/packdraw Prizes for Ranks 4-15** - Prizes were showing as $0 instead of $300/$200/$150/$75/$50
   - Root cause: `page-scraper.js` Turndown config was missing table rules for HTMLâ†’Markdown conversion
   - HTML `<table>` elements weren't being converted to proper markdown table format (`| cell |`)
   - This caused `parseMarkdownTables()` to find 0 entries, falling back to OCR which has no prize extraction
   - **Fix**: Added table rules (`tableCell`, `tableRow`) and cleanup rules to Turndown in `page-scraper.js`
   - Now correctly extracts all 15 prize positions ($1500, $800, $500, $300, $200, $150, $75x4, $50x5)

2. **Historical API Filtering** - Added filtering for past-winners/historical leaderboard APIs
   - URLs containing `past-winners`, `previous-leaderboard`, `history`, or `year=YYYY&month=` patterns are now filtered
   - Prevents mixing current leaderboard data with historical data

3. **Debug Logging for Markdown Extraction** - Added automatic debug output for devlrewards/packdraw
   - Saves markdown content to `/debug/` folder for analysis
   - Shows sample entries and prize counts from each parser

### Files Modified in v7.26
- `/Users/cj/Desktop/ARGUS/lbscraper/core/page-scraper.js` - Added Turndown table rules for proper HTMLâ†’Markdown conversion
- `/Users/cj/Desktop/ARGUS/lbscraper/core/data-fusion.js` - Pass siteName to strategies for debugging
- `/Users/cj/Desktop/ARGUS/lbscraper/strategies/markdown-extraction.js` - Added debug logging for packdraw/devlrewards
- `/Users/cj/Desktop/ARGUS/lbscraper/strategies/api-merger.js` - Added historical URL filtering patterns
- `/Users/cj/Desktop/ARGUS/lbscraper/orchestrators/scrape-orchestrator.js` - Skip historical API pagination

### Reference Sites (4 sites, yeeterboards.com removed)
- **paxgambles.com**: 3 LBs, 10 entries each
- **devlrewards.com**: 1 LB, ~19 entries
- **goatgambles.com**: 5 LBs, 10-25 entries each
- **wrewards.com**: 4 LBs, 25-50 entries each (HUNT #1 = $30,000)

### Important Context Files
- **This file**: `/Users/cj/Desktop/ARGUS/scraper_details.md` - Architecture & history
- **Websites list**: `/Users/cj/Desktop/ARGUS/lbscraper/websites.txt` - 500+ sites to scrape
- **Main scraper**: `/Users/cj/Desktop/ARGUS/lbscraper/new-run-scraper.js` - Entry point

### Claude Memory/Session Files
- **Session transcript**: `/Users/cj/.claude/projects/-Users-cj-Desktop-ARGUS/7b7ab150-a7b7-47bb-aed2-40d53d5f478d.jsonl`
- **Plan file (v7.25)**: `/Users/cj/.claude/plans/glistening-launching-prism.md`

### Key Technical Details to Remember
1. **3-Pillar Architecture**: Crawler â†’ Scraper â†’ Extractor (never mix responsibilities)
2. **Data Fusion**: Merges API, DOM, Markdown, Geometric, OCR sources with confidence scoring
3. **wrewards.com Special Case**: Uses `custom-reward` div for special prizes (HTML-only, not in API)
4. **Stealth Mode**: Uses `puppeteer-extra-plugin-stealth` to bypass bot detection
5. **Network Capture**: Intercepts API responses during page load for data extraction

---

# âš ï¸ CRITICAL WARNING FOR AI AGENTS âš ï¸

## STOP! READ THIS BEFORE MAKING ANY CHANGES TO EXTRACTION LOGIC

**This scraper handles 500+ sites with DIFFERENT formats. A "fix" for one site WILL break other sites.**

### FORBIDDEN ACTIONS (will break sites):
- âŒ Adding assumptions about data ordering (wager vs prize)
- âŒ Swapping values based on which is larger
- âŒ Changing deduplication keys
- âŒ Modifying rank filtering ranges
- âŒ Assuming ANY site-specific pattern is universal

### MANDATORY REGRESSION TEST PROTOCOL

**BEFORE committing ANY extraction changes, you MUST:**

```bash
node lbscraper/new-run-scraper.js https://paxgambles.com/leaderboard https://devlrewards.com/leaderboard https://goatgambles.com/leaderboard https://wrewards.com/leaderboards
```

### EXPECTED VALUES (if ANY of these fail, DO NOT COMMIT):

| Site | Leaderboards | Entries per LB | Notes |
|------|--------------|----------------|-------|
| paxgambles.com | 3 | 10 each | NOT 11 (no duplicates) |
| goatgambles.com | 5 | 10-20 each | All LBs must work |
| wrewards.com | 4 | 25-50 each | HUNT #1 must be $30,000 prize |

### THE PATTERN OF FAILURE (learn from history):

1. Site X has a problem
2. AI adds logic that assumes "all sites work like site X"
3. Sites Y and Z break because they work differently
4. This happened in v7.19-v7.21 and broke 4 working sites

### IF YOU BREAK A WORKING SITE, YOU HAVE FAILED.

---

## Table of Contents

1. [Overview](#overview)
2. [Directory Structure](#directory-structure)
3. [The 3 Pillars (Core Modules)](#the-3-pillars-core-modules)
4. [Extraction Strategies](#extraction-strategies)
5. [Orchestrators](#orchestrators)
6. [Shared Modules](#shared-modules)
7. [Data Flow](#data-flow)
8. [Database Schema](#database-schema)
9. [Admin API](#admin-api)
10. [Critical Rules - NEVER Violate](#critical-rules---never-violate)
11. [File Size Limits](#file-size-limits)
12. [Adding New Features](#adding-new-features)
13. [Common AI Mistakes](#common-ai-mistakes-to-avoid)

---

## Overview

The scraper extracts leaderboard data from gambling affiliate websites. It:
- Discovers leaderboards on a site (via URL patterns, button clicks, API interception)
- Collects raw data (HTML, API responses, screenshots)
- Extracts structured entries (rank, username, wager, prize)
- Validates and scores confidence
- Saves to PostgreSQL database

**Scale**: 500+ sites, scraped every 60 minutes

**Key Principle**: Separation of concerns. Each module has ONE responsibility.

---

## Directory Structure

```
lbscraper/
â”‚
â”œâ”€â”€ core/                           # THE 3 PILLARS (core extraction logic)
â”‚   â”œâ”€â”€ index.js                    # Exports all core modules
â”‚   â”œâ”€â”€ leaderboard-crawler.js      # DISCOVERS leaderboards (369 lines)
â”‚   â”œâ”€â”€ page-scraper.js             # COLLECTS raw data (354 lines)
â”‚   â””â”€â”€ data-extractor.js           # PARSES entries (157 lines)
â”‚
â”œâ”€â”€ strategies/                     # PLUGGABLE extraction methods
â”‚   â”œâ”€â”€ index.js                    # Exports strategies, DEFAULT_STRATEGIES
â”‚   â”œâ”€â”€ api-extraction.js           # Priority 1 - Parse API responses (379 lines)
â”‚   â”œâ”€â”€ api-merger.js               # Split API response detection & merging (567 lines)
â”‚   â”œâ”€â”€ dom-extraction.js           # Priority 2 - Parse HTML tables (348 lines)
â”‚   â”œâ”€â”€ geometric-extraction.js     # Priority 3 - Visual/spatial analysis (411 lines)
â”‚   â””â”€â”€ ocr-extraction.js           # Priority 4 - Screenshot OCR (286 lines)
â”‚
â”œâ”€â”€ orchestrators/                  # HIGH-LEVEL coordination
â”‚   â”œâ”€â”€ index.js                    # Exports orchestrators
â”‚   â”œâ”€â”€ scrape-orchestrator.js      # Coordinates 3 pillars for 1 site (329 lines)
â”‚   â””â”€â”€ batch-runner.js             # Runs multiple sites (327 lines)
â”‚
â”œâ”€â”€ shared/                         # SHARED utilities (DO NOT put business logic here)
â”‚   â”œâ”€â”€ utils.js                    # Logging, parsing, file I/O
â”‚   â”œâ”€â”€ config.js                   # Configuration loading
â”‚   â”œâ”€â”€ db-save.js                  # Database persistence (auto-save scrape results)
â”‚   â”œâ”€â”€ json-logger.js              # Debug JSON logging with auto-cleanup
â”‚   â”œâ”€â”€ network-capture.js          # Playwright network interception
â”‚   â”œâ”€â”€ page-navigation.js          # Navigation with bypass
â”‚   â”œâ”€â”€ site-detection.js           # Switcher/button detection
â”‚   â”œâ”€â”€ entry-validation.js         # Confidence scoring, cleaning
â”‚   â”œâ”€â”€ extraction.js               # Legacy extraction helpers (being deprecated)
â”‚   â”œâ”€â”€ api-patterns.js             # Learned API URL patterns
â”‚   â”œâ”€â”€ api-exploiter.js            # API parameter discovery
â”‚   â”œâ”€â”€ keyword-cache.js            # Per-domain keyword storage
â”‚   â””â”€â”€ learned-patterns.js         # Persisted extraction configs
â”‚
â”œâ”€â”€ teacher/                        # LLM TEACHER MODE (self-contained, don't touch)
â”‚   â”œâ”€â”€ teacher-mode.js             # Main teacher orchestration
â”‚   â”œâ”€â”€ site-profiles.js            # Per-site learning
â”‚   â”œâ”€â”€ cost-tracker.js             # LLM budget management
â”‚   â”œâ”€â”€ visual-verifier.js          # Screenshot verification
â”‚   â””â”€â”€ ... (12 files total)
â”‚
â”œâ”€â”€ data/                           # Runtime data (cache, known links)
â”œâ”€â”€ results/                        # Output JSON files
â”‚
â”œâ”€â”€ new-run-scraper.js              # Entry point using modular architecture
â”œâ”€â”€ challenge-bypass.js             # Cloudflare/hCaptcha bypass module
â”œâ”€â”€ teacher-admin.js                # Admin interface for teacher mode
â”œâ”€â”€ link-discovery.js               # Link discovery utility
â”œâ”€â”€ keywords.txt                    # Keywords list for site detection
â””â”€â”€ websites.txt                    # Sites list for batch scraping
```

---

## The 3 Pillars (Core Modules)

### 1. Leaderboard Crawler (`core/leaderboard-crawler.js`)

**Responsibility**: DISCOVER leaderboards on a site

**What it does**:
- Scans multiple URL paths (/leaderboard, /leaderboards, etc.)
- Finds clickable switchers (buttons, tabs, dropdowns)
- Detects URL patterns for direct navigation
- Groups switchers by spatial proximity

**Input**:
```javascript
{
  page,              // Playwright page
  baseUrl,           // e.g., "https://example.com"
  keywords,          // ["gamdom", "stake", "packdraw", ...]
  config: {
    waitAfterLoad,   // ms to wait for dynamic content
    takeScreenshots  // capture screenshots for LLM verification
  }
}
```

**Output**:
```javascript
{
  leaderboardUrls: [{ name, url, method }],  // Direct URL leaderboards
  switchers: [{ keyword, type, coordinates }], // Click-based switchers
  historicalPaths: [],                         // Past leaderboard URLs
  bestPath: "/leaderboards",                   // Best path found
  urlPattern: { pattern, baseUrl },            // Detected URL pattern
  errors: []
}
```

**NEVER** put extraction logic here. This module only DISCOVERS.

---

### 2. Page Scraper (`core/page-scraper.js`)

**Responsibility**: COLLECT raw data from a page

**What it does**:
- Captures HTML content
- Converts to markdown for analysis
- Intercepts API calls and stores responses
- Takes screenshots
- Scrolls to trigger lazy loading

**Input**:
```javascript
{
  page,              // Playwright page (already navigated)
  url,               // Current URL
  networkData,       // From setupNetworkCapture()
  config: {
    takeScreenshot,  // boolean
    scrollPage,      // boolean
    waitForContent   // ms
  }
}
```

**Output**:
```javascript
{
  html: "...",                    // Raw HTML
  markdown: "...",                // Converted markdown
  apiCalls: ["url1", "url2"],     // API URLs captured
  rawJsonResponses: [{url, data, timestamp}], // Full API responses
  screenshot: Buffer | null,      // PNG screenshot
  metadata: { url, title, viewport },
  errors: []
}
```

**NEVER** put parsing/extraction logic here. This module only COLLECTS.

---

### 3. Data Extractor (`core/data-extractor.js`)

**Responsibility**: PARSE raw data into structured entries

**What it does**:
- Tries extraction strategies in priority order
- Validates and cleans entries
- Calculates confidence scores
- Returns best result

**Input**:
```javascript
{
  html,                // From page-scraper
  markdown,            // From page-scraper
  apiCalls,            // From page-scraper
  rawJsonResponses,    // From page-scraper
  screenshot,          // For OCR fallback
  page,                // For dynamic extraction
  siteName,            // e.g., "gamdom"
  config: {
    minConfidence,     // Minimum acceptable confidence (default 50)
    strategies         // Optional custom strategy list
  }
}
```

**Output**:
```javascript
{
  entries: [{ rank, username, wager, prize }],
  prizes: [{ rank, prize }],           // Prize table
  confidence: 85,                       // 0-100
  extractionMethod: "api",              // Which strategy succeeded
  metadata: { strategiesTried: [], apiUrl },
  errors: []
}
```

**NEVER** put navigation or data collection here. This module only PARSES.

---

## Extraction Strategies

Strategies are **pluggable** extraction methods. They are tried in priority order until one succeeds with sufficient confidence.

### Strategy Interface

Every strategy must implement:
```javascript
{
  name: 'api',           // Strategy identifier
  priority: 1,           // Lower = tried first

  canExtract(input) {    // Returns boolean
    // Check if this strategy can work with the input
    return input.rawJsonResponses?.length > 0;
  },

  async extract(input) { // Returns result or null
    // Perform extraction
    return {
      entries: [...],
      prizes: [...],
      confidence: 85
    };
  }
}
```

### Priority Order

| Priority | Strategy | When to Use |
|----------|----------|-------------|
| 1 | `api-extraction.js` | API responses captured |
| 1.5 | `markdown-extraction.js` | Markdown content with podium/list patterns |
| 2 | `dom-extraction.js` | HTML tables/lists present |
| 3 | `geometric-extraction.js` | Visual structure detected |
| 4 | `ocr-extraction.js` | Screenshot available (last resort) |

**Note**: Markdown extraction (priority 1.5) runs after API but before DOM. It handles podium layouts (1st/2nd/3rd with prizes) and ranked lists from markdown content.

### API Response Merger (`strategies/api-merger.js`)

The API merger is a **pre-processing step** that runs before extraction strategies. It handles sites with split APIs where user data and prize data come from separate endpoints.

**What it does**:
- Categorizes API responses by content type (users, prizes, combined, unknown)
- Detects numbered prize fields (`prize1`, `prize2`, `prize3` pattern)
- Detects `additionalPrizes` arrays with `{prizeNumber, amount}` structure
- Merges user entries with prize data from separate responses
- Injects prizes into entries based on rank matching

**URL Patterns Detected**:
| Type | Patterns |
|------|----------|
| Users | `/ld-leaders`, `/leaders?...viewState`, `/leaderboard.*entries` |
| Prizes | `/leaderboard-info?`, `/list-winner`, `/prize.*pool`, `/payouts` |
| Combined | `/leaderboard/data`, `/leaderboard/full`, `/leaderboard/current` |

**Response Types Supported**:
```javascript
// User entries (wrewards.com pattern)
{ "data": [{ "displayName", "wageredTotal", "position" }] }

// Leaderboard metadata with prizes
{ "data": { "prize1": 25000, "prize2": 10000, "additionalPrizes": [...], "totalPrizePool": 50000 } }

// Past leaderboards
{ "items": [{ "prize1", "totalPrizePool", "winner" }] }
```

**Detection Priority Order** (in `analyzeDataContent()`):
1. Check if `data` is an array of user entries (FIRST - prevents miscategorization)
2. Check if `data` is a metadata object with numbered prizes
3. Check for prize table keys (`prizes`, `prizeTable`, `rewards`, etc.)
4. Check for numbered prize fields directly (`prize1`, `prize2`)
5. Recursively check nested objects

### Adding a New Strategy

1. Create `strategies/my-extraction.js`
2. Implement the strategy interface
3. Export in `strategies/index.js`
4. Add to `DEFAULT_STRATEGIES` array

**NEVER** modify `data-extractor.js` to add extraction logic directly. Always use strategies.

---

## Orchestrators

### Scrape Orchestrator (`orchestrators/scrape-orchestrator.js`)

**Responsibility**: Coordinate the 3 pillars for a SINGLE site

**Features**:
- Retry logic (3x exponential backoff)
- Circuit breaker (prevents hammering failed sites)
- Calls: crawler â†’ scraper â†’ extractor in sequence
- Teacher Mode fallback when extraction fails

**Teacher Mode Fallback**:
When extraction fails (0 entries), the orchestrator invokes Teacher Mode:
1. Checks `shouldInvokeTeacherMode(profile, confidenceScore)`
2. If approved, runs `evaluateWithTeacher()`:
   - Phase 1: Quick screenshot analysis
   - Phase 2: Interactive browser exploration (if Phase 1 fails)
3. Results marked with `source: "teacher-phase1"` or `"teacher-phase2"`

Budget limits are enforced via `shared/teacher/cost-tracker.js`.

**Usage**:
```javascript
const result = await orchestrateScrape({
  page,
  baseUrl: 'https://example.com',
  networkData,
  config,
  keywords
});
```

### Batch Runner (`orchestrators/batch-runner.js`)

**Responsibility**: Run scraper across MULTIPLE sites

**Features**:
- Loads sites from database or file
- Respects refresh intervals per site
- Graceful shutdown handling
- Progress tracking

**Usage**:
```javascript
await runBatch({
  production: true,
  maxWorkers: 2,
  delayBetweenSitesMs: 5000
});
```

### Scalable Scraper Architecture (PDF) Alignment

The project implements recommendations from the *Scalable Scraper Architecture* PDF:

| PDF recommendation | Implementation |
|--------------------|----------------|
| **Resource blocking** | `shared/resource-blocking.js`: blocks images, fonts, media, trackers. Toggle: `SCRAPER_BLOCK_RESOURCES` (default true). |
| **domcontentloaded + selector wait** | `page-navigation.js` `navigateWithBypass()`: uses `waitUntil: 'domcontentloaded'`, then waits for leaderboard-like selector (table, .leaderboard, .ranking, .challenger) with 10s timeout before proceeding. |
| **Parallel execution** | Batch mode uses `SCRAPER_MAX_WORKERS` (default 3). Multiple sites run in parallel via `Promise.all` in `batch-runner.js`. |
| **Wait for dropdown/options** | `ui-interaction.js`: before clicking custom dropdown, waits for trigger to be visible; before selecting option, waits for `[role="listbox"]` / options to appear. |
| **Wait for new rows after "Show More"** | `scrape-orchestrator.js`: after each "Show More" click, waits for table/row selector (3s) instead of blind sleep only. |
| **Retries, timeouts, error handling** | Existing: `withRetry`, circuit breaker, per-site errors logged; batch continues on failure. |

### Periodic Re-Discovery

Sites may add new leaderboards at any time. The scraper should periodically re-run discovery even for sites with existing configurations:

**Re-discovery Frequency**: Every 24 hours

**Why**: New casino partnerships, seasonal leaderboards, or site redesigns can add new tabs/buttons that weren't present before. Relying only on cached configs will miss these additions.

**Implementation Note**: The scraper should check `lastDiscoveryAt` timestamp and force a fresh `discoverLeaderboards()` call if more than 24 hours have passed, regardless of existing config.

### Non-Standard Leaderboard URLs

Not all sites use `/leaderboard` or `/leaderboards` paths. Some patterns:

| Site Type | URL Pattern | Notes |
|-----------|-------------|-------|
| Standard | `/leaderboard`, `/leaderboards` | Most common |
| Homepage | `/` (main page) | Leaderboard embedded in homepage |
| Casino subdomain | `leaderboard.site.com` | Separate subdomain |
| Query param | `/?section=leaderboard` | SPA with query routing |
| Hash routing | `/#/leaderboards` | React/Angular apps |

The crawler's `findLeaderboardPage()` function should check all these patterns.

---

## Shared Modules

| File | Purpose | Notes |
|------|---------|-------|
| `utils.js` | Logging, parsing, file I/O | Pure utilities only |
| `config.js` | Load configuration | Database + env + defaults |
| `db-save.js` | Database persistence | Auto-save scrape results to PostgreSQL |
| `json-logger.js` | Debug JSON logging | Timestamped files with auto-cleanup |
| `network-capture.js` | Intercept network requests | Playwright-specific |
| `page-navigation.js` | Navigate with bypass | Handles Cloudflare |
| `resource-blocking.js` | Block images/fonts/media/trackers | PDF perf; toggle via SCRAPER_BLOCK_RESOURCES |
| `site-detection.js` | Find switchers/buttons | Word boundary matching for keywords |
| `entry-validation.js` | Clean/validate entries | Confidence scoring |
| `extraction.js` | Currency parsing, markdown helpers | Multi-currency support |
| `api-patterns.js` | Learned URL patterns, prize table extraction | Per-site storage, supports `additionalPrizes` format |
| `keyword-cache.js` | Domain keyword storage | Discovered keywords |

**Rule**: Shared modules are UTILITIES. They should not contain business logic for specific extraction methods.

**Keyword Detection**: Uses word boundary regex matching. Example: "rain" matches "rain.gg" but NOT "rainbet" or "terrain". Detected sites require clickable element evidence before being accepted as leaderboards.

---

## Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BATCH RUNNER                                â”‚
â”‚  Load sites from DB â†’ Filter due sites â†’ Process in parallel     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SCRAPE ORCHESTRATOR                            â”‚
â”‚  Coordinates 3 pillars, handles retries, circuit breaker         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CRAWLER       â”‚ â”‚   SCRAPER   â”‚ â”‚   EXTRACTOR + FUSION (v7.4)   â”‚
â”‚                 â”‚ â”‚             â”‚ â”‚                               â”‚
â”‚ Find switchers  â”‚ â”‚ Get HTML    â”‚ â”‚  API Merger (pre-process):    â”‚
â”‚ Detect patterns â”‚ â”‚ Capture API â”‚ â”‚   Detect response types       â”‚
â”‚ Discover URLs   â”‚ â”‚ Screenshot  â”‚ â”‚   Merge users + prizes        â”‚
â”‚                 â”‚ â”‚             â”‚ â”‚                               â”‚
â”‚                 â”‚ â”‚             â”‚ â”‚  Run ALL strategies:          â”‚
â”‚                 â”‚ â”‚             â”‚ â”‚   API â”€â”€â”€â”€â”€â”                  â”‚
â”‚                 â”‚ â”‚             â”‚ â”‚   Markdown â”œâ”€â–º Cross-Validate â”‚
â”‚                 â”‚ â”‚             â”‚ â”‚   DOM â”€â”€â”€â”€â”€â”¤    â”œâ”€â–º Fuse      â”‚
â”‚                 â”‚ â”‚             â”‚ â”‚   OCR â”€â”€â”€â”€â”€â”˜    â”œâ”€â–º Filter    â”‚
â”‚                 â”‚ â”‚             â”‚ â”‚                 â””â”€â–º Learn     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                    â”‚
         â”‚ URLs            â”‚ RawData           â”‚ Fused Entries
         â””â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ENTRY VALIDATION                               â”‚
â”‚  Clean usernames, score confidence, detect duplicates            â”‚
â”‚  + Cross-validation bonus/penalty based on source agreement      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATABASE                                    â”‚
â”‚  LeaderboardSite â†’ LeaderboardCycle â†’ LeaderboardSnapshot        â”‚
â”‚                                            â†’ LeaderboardEntry    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Leaderboard Extraction Engine Architecture (Reliability Layers)

The scraper is structured as a **leaderboard extraction engine with heuristics + validation**:

```
Navigator (page-navigation, leaderboard-crawler, ui-interaction)
   â†“
Dataset Loader (page-scraper: scrollUntilStable, max-rows selection, readiness)
   â†“
Extraction Strategies (API â†’ Markdown â†’ DOM â†’ Geometric â†’ OCR, parallel + fusion)
   â†“
Validation Layer (dataset-validation, entry-validation, validateLeaderboardData)
   â†“
Normalizer (shared/normalizer.js: rank, username, wager, prize, timestamp, leaderboard_type)
   â†“
Database (db-save)
```

**New/updated modules (reliability)**:
- **shared/ui-interaction.js**: Leaderboard tab/dropdown/pagination detection (heuristic); max-rows selection; leaderboard readiness (stable DOM, row count); retry for UI actions.
- **shared/dataset-validation.js**: Dataset completeness (ranks sequential, no duplicates, min rows); data sanity (wager/prize â‰¥ 0, non-empty usernames); strategy agreement â†’ low confidence when strategies disagree.
- **shared/normalizer.js**: Standard schema normalization (rank formats, currency, masked usernames, leaderboard_type).
- **core/page-scraper.js**: `scrollUntilStable()` for full dataset capture; config `scrollUntilStable` / `scrollUntilStableOptions`.
- **orchestrators/scrape-orchestrator.js**: Calls `waitForLeaderboardReady`, `selectMaxRows` (with retry), uses `scrollUntilStable`, runs `validateDataset`, applies `confidencePenalty`, outputs `validation` and normalized entries.
- **core/data-fusion.js**: When confidence is tied, primary method prefers API > markdown > dom > geometric > ocr.

---

## Database Schema

```
LeaderboardSite (one per domain)
â”œâ”€â”€ id, domain, name, isActive
â”œâ”€â”€ refreshInterval, useGlobalInterval
â”œâ”€â”€ lastScrapedAt, lastError, errorCount
â”‚
â”œâ”€â”€ LeaderboardConfig[] (per-site configs like "gamdom", "stake")
â”‚   â”œâ”€â”€ siteName, accessMethod, extractionMethod
â”‚   â”œâ”€â”€ urlPatterns, siteKeywords, confidence
â”‚   â””â”€â”€ validated
â”‚
â””â”€â”€ LeaderboardCycle[] (one competition period)
    â”œâ”€â”€ siteName, cycleNumber, startedAt, endedAt
    â”œâ”€â”€ timerDuration, prizePool
    â”‚
    â””â”€â”€ LeaderboardSnapshot[] (point-in-time captures)
        â”œâ”€â”€ timer, prizePool, totalWager
        â”œâ”€â”€ confidence, extractionMethod
        â”‚
        â””â”€â”€ LeaderboardEntry[] (individual player entries)
            â”œâ”€â”€ rank, username, wager, prize
            â””â”€â”€ verified
```

---

## Data Output Format

### Unique Identifiers

Every extraction includes hierarchical UUIDs for database tracking:

```javascript
{
  "id": "uuid-for-extraction-run",
  "results": [{
    "id": "uuid-for-leaderboard",
    "extractionId": "uuid-for-extraction-run",  // Parent reference
    "entries": [{
      "id": "uuid-for-entry",
      "extractedAt": "2026-01-25T21:28:36.174Z"
    }],
    "scrapedAt": "2026-01-25T21:28:36.175Z"
  }],
  "metadata": {
    "startedAt": "...",
    "completedAt": "..."
  }
}
```

**Purpose**: IDs enable database primary keys, historical lookups, deduplication, and audit trails.

### Currency Handling

The scraper handles various currency formats via `parseMarkdownAmount()` in `shared/extraction.js`:

| Type | Examples |
|------|----------|
| Symbols | $, â‚¬, Â£, Â¥, â‚¹, à¸¿, â‚¿, â—†, â™¦ |
| Emojis | ğŸ’°, ğŸª™, ğŸ’, â­, âœ¨, ğŸ†, ğŸ–ï¸, ğŸ¥‡, ğŸ¥ˆ, ğŸ¥‰ |
| Text Labels | coins, credits, points, gems, tokens, chips, diamonds, stars |
| Multipliers | K (thousands), M (millions), B (billions) |

All are normalized to numeric values before storage.

---

## Admin API

Base path: `/admin/scraper`

| Method | Endpoint | Purpose |
|--------|----------|---------|
| GET | `/sites` | List all sites (paginated) |
| GET | `/sites/:id` | Get single site |
| PUT | `/sites/:id` | Update site config |
| POST | `/sites/:id/run` | Trigger manual scrape |
| POST | `/sites/:id/clear-errors` | Clear error state |
| GET | `/stats` | Overall statistics |
| GET | `/due` | Sites due for scraping |
| POST | `/pause` | Pause all workers |
| POST | `/resume` | Resume workers |
| GET | `/config/json-logging` | Get JSON logging config + storage stats |
| PUT | `/config/json-logging` | Update JSON logging config |
| GET | `/logs/stats` | Get log storage statistics |
| POST | `/logs/cleanup` | Trigger manual log cleanup |

Location: `src/modules/scraper/`

---

## Critical Rules - NEVER Violate

### 1. Separation of Concerns

```
âŒ WRONG: Adding extraction logic to page-scraper.js
âŒ WRONG: Adding navigation logic to data-extractor.js
âŒ WRONG: Adding DOM parsing to leaderboard-crawler.js

âœ… RIGHT: Crawler DISCOVERS, Scraper COLLECTS, Extractor PARSES
```

### 2. Strategy Pattern

```
âŒ WRONG: Adding new extraction method directly to data-extractor.js
âŒ WRONG: Modifying existing strategies to handle new site types

âœ… RIGHT: Create new strategy file in strategies/
âœ… RIGHT: Add to DEFAULT_STRATEGIES array
```

### 3. File Size Limits

```
âŒ WRONG: Any file over 600 lines
âŒ WRONG: Mixing multiple responsibilities in one file

âœ… RIGHT: Split large files into focused modules
âœ… RIGHT: Each file has ONE clear responsibility
```

### 4. Import Structure

```
âŒ WRONG: Circular imports between core modules
âŒ WRONG: Orchestrators importing from teacher/

âœ… RIGHT: core/ imports from shared/
âœ… RIGHT: strategies/ imports from shared/
âœ… RIGHT: orchestrators/ imports from core/ and shared/
```

### 5. Data Contracts

```
âŒ WRONG: Changing input/output shapes without updating docs
âŒ WRONG: Adding optional fields without defaults

âœ… RIGHT: Input/output shapes are documented
âœ… RIGHT: New fields have sensible defaults
```

### 6. Teacher Module

```
âŒ WRONG: Modifying teacher/ files without understanding the system
âŒ WRONG: Importing teacher functions into core modules

âœ… RIGHT: Teacher module is self-contained
âœ… RIGHT: Only orchestrators may call teacher functions
```

### 7. NEVER BREAK WORKING SITES (Regression Prevention)

```
âŒ WRONG: Fixing site X in a way that assumes all sites work like X
âŒ WRONG: Swapping wager/prize based on which value is larger
âŒ WRONG: Changing deduplication keys without testing ALL sites
âŒ WRONG: Committing extraction changes without running regression tests

âœ… RIGHT: Changes must be ADDITIVE (new patterns), not DESTRUCTIVE (changing existing behavior)
âœ… RIGHT: Always test on ALL 5 reference sites before committing
âœ… RIGHT: If one site breaks, the change is WRONG - find another approach
âœ… RIGHT: Use labeled data (Wagered:, Prize:) when available, never assume ordering
```

**MANDATORY TEST COMMAND:**
```bash
node lbscraper/new-run-scraper.js https://paxgambles.com/leaderboard https://devlrewards.com/leaderboard https://goatgambles.com/leaderboard https://wrewards.com/leaderboards https://yeeterboards.com/leaderboards
```

**Historical failures from violating this rule:**
- v7.19: Changed dedup key â†’ broke paxgambles
- v7.20: Added swap logic â†’ broke paxgambles, devlrewards
- v7.21: Added complexity â†’ compounded failures

---

## File Size Limits

Per PROJECT_RULES.md Rule 8:

| Module Type | Max Lines | Rationale |
|-------------|-----------|-----------|
| core/*.js | 300-500 | AI can understand in one read |
| strategies/*.js | 150-400 | Single extraction method |
| orchestrators/*.js | 200-400 | Coordination only |
| shared/*.js | 300-600 | Focused utilities |

**If a file exceeds limits**: Split into smaller, focused modules.

---

## Adding New Features

### New Extraction Method

1. Create `strategies/my-extraction.js`
2. Implement strategy interface (name, priority, canExtract, extract)
3. Export in `strategies/index.js`
4. Add to `DEFAULT_STRATEGIES` array
5. Document in this file

### New Site Type

1. Add keywords to `keywords.txt`
2. If special handling needed, create strategy
3. Test with `node new-run-scraper.js <url>`

### New API Endpoint

1. Add to `src/modules/scraper/scraper.service.ts`
2. Add route to `scraper.controller.ts`
3. Create DTO if needed
4. Document in this file

### New Shared Utility

1. Add to appropriate file in `shared/`
2. If file gets too large, split it
3. **Never add business logic to shared/**

---

## Common AI Mistakes to Avoid

### 1. Mixing Core Module Responsibilities

**Wrong**:
```javascript
// In page-scraper.js
const entries = parseLeaderboardTable(html); // NO! This is extraction
```

**Right**:
```javascript
// In page-scraper.js
return { html, markdown, apiCalls }; // Just collect

// In data-extractor.js (via strategies)
const entries = strategy.extract(input); // Extract here
```

### 2. Hardcoding Site-Specific Logic

**Wrong**:
```javascript
// In api-extraction.js
if (url.includes('gamdom')) {
  // Special gamdom handling
}
```

**Right**:
```javascript
// Create strategies/gamdom-extraction.js if truly needed
// Or add to site-profiles.js for learned patterns
```

### 3. Modifying Legacy Code

**Wrong**:
```javascript
// Editing current-scraper.js to add features
```

**Right**:
```javascript
// Add features to the new modular architecture
// current-scraper.js is deprecated
```

### 4. Forgetting Input Validation

**Wrong**:
```javascript
async extract(input) {
  const data = input.rawJsonResponses[0].data; // Crashes if empty
}
```

**Right**:
```javascript
async extract(input) {
  if (!input.rawJsonResponses?.length) return null;
  const data = input.rawJsonResponses[0].data;
}
```

### 5. Breaking the Strategy Interface

**Wrong**:
```javascript
// Adding extra required parameters
async extract(input, page, config) { ... }
```

**Right**:
```javascript
// Everything goes in input object
async extract(input) {
  const { page, config } = input;
}
```

### 6. Not Clearing Network Data Between Leaderboard Switches

**Wrong**:
```javascript
// Click switcher without clearing network data first
await clickSwitcher(page, keyword, switcherData);
// rawJsonResponses now contains ALL responses from entire session
```

**Right**:
```javascript
// Clear network data BEFORE clicking to capture only THIS leaderboard's responses
clearNetworkData(networkData);
await clickSwitcher(page, keyword, switcherData);
// rawJsonResponses now contains only responses from this click
```

**Why**: The `setupNetworkCapture()` listener accumulates ALL responses during the session. If you don't clear before each switcher click, the API extraction will pick from stale responses.

### 7. Using Coordinates for Click-Based Navigation

**Wrong**:
```javascript
// Relying only on coordinate clicks
await page.mouse.click(coords.x, coords.y);
// Might miss if page layout shifted or element not interactable
```

**Right**:
```javascript
// Try image-based clicks first for sites with logo buttons
const imageClick = await page.evaluate((kw) => {
  const img = document.querySelector(`img[src*="${kw}"], img[alt*="${kw}"]`);
  if (img) {
    const clickable = img.closest('button, a, [role="button"]');
    if (clickable) { clickable.click(); return true; }
  }
  return false;
}, keyword);

// Fall back to coordinates only if image click fails
if (!imageClick && coords) {
  await page.mouse.click(coords.x, coords.y);
}
```

**Why**: Many sites use logo images as tab buttons (e.g., wrewards.com with gamdom/packdraw/lootbox tabs). Finding and clicking the image's parent element is more reliable than coordinate-based clicks.

### 8. Not Validating Single-Source Entries

**Wrong**:
```javascript
// Accepting all entries from a single source without validation
const fusedEntries = allEntries;
```

**Right**:
```javascript
// Filter suspicious single-source entries that have duplicate wagers
const cleanedEntries = fusedEntries.filter(entry => {
  if (entry._fusion?.sources?.length > 1) return true; // Multi-source = verified
  const wager = Math.round(entry.wager);
  if (wager > 0 && wagerCounts.get(wager) > 1) {
    // Duplicate wager from single source = likely parsing bug
    return false;
  }
  return true;
});
```

**Why**: DOM extraction can produce "Frankenstein" entries that mix username from one user, wager from another, and rank from a third. If a single-source entry has the exact same wager as another entry, it's almost certainly a parsing bug.

### 9. Not Respecting URL-Specific Leaderboards

**Wrong**:
```javascript
// Always running discovery regardless of URL
const discovery = await discoverLeaderboards(page, keywords);
// Scrapes ALL leaderboards even when URL was /leaderboard/csgogem
```

**Right**:
```javascript
// Check if URL specifies a particular leaderboard
const urlKeyword = extractKeywordFromPath(url);
if (urlKeyword) {
  // Skip discovery, only scrape this specific leaderboard
  return [{ name: urlKeyword, url, method: 'url-specified' }];
}
// Otherwise run full discovery
const discovery = await discoverLeaderboards(page, keywords);
```

**Why**: Sites like betjuicy.com have separate URLs for each leaderboard (`/leaderboard/csgogem`, `/leaderboard/roobet`). If we run discovery on each URL, we'd scrape all leaderboards multiple times with identical data.

---

## Quick Reference

### Environment Variables

| Variable | Default | Purpose |
|----------|---------|---------|
| `SCRAPER_HEADLESS` | `true` | Set to `false` to show browser for debugging |
| `ANTHROPIC_API_KEY` | - | Required for Teacher Mode LLM integration |

### Run Single Site (headless - default)
```bash
node lbscraper/new-run-scraper.js https://example.com/leaderboards
```

### Run Single Site (visible browser for debugging)
```bash
SCRAPER_HEADLESS=false node lbscraper/new-run-scraper.js https://example.com/leaderboards
```

### Run Batch
```bash
node lbscraper/new-run-scraper.js --batch --production
```

### Check Syntax
```bash
node -c lbscraper/core/data-extractor.js
```

### Key Imports
```javascript
// Core modules
const { discoverLeaderboards } = require('./core');
const { scrapePageData } = require('./core');
const { extractLeaderboardData } = require('./core');

// Orchestrators
const { orchestrateScrape } = require('./orchestrators');
const { runBatch } = require('./orchestrators');

// Strategies
const { DEFAULT_STRATEGIES } = require('./strategies');
```

---

## Version History

- **v7.29** (Feb 3 2026): Fusion & Prize Extraction Fixes
  - **FIXED: DOM entries incorrectly filtered** (`core/data-fusion.js`)
    - Problem: Fusion used markdown's max rank as ceiling to filter DOM entries
    - Example: scrapesgambles.com - markdown found 3 entries, DOM found 11 entries
    - DOM entries 4-11 were filtered as "beyond max rank 3 from single source dom"
    - Added: Trust high-confidence DOM (>=85%) when it finds MORE entries than markdown
    - Result: scrapesgambles.com now extracts 10 entries instead of 3
  - **FIXED: Prizes not extracted for ranks 4-10** (`shared/extraction.js`)
    - Problem: List parser only handled `Prize: VALUE` on same line
    - Many sites have "Prize" on one line, amount on next: `Prize\n![Currency]200`
    - Added: Handling for "Prize" and "Wagered" labels without colon (amount on next line)
    - Result: tanskidegen.com now extracts prizes for all 10 entries instead of just top 3
  - **Tested sites**: scrapesgambles.com âœ…, tanskidegen.com âœ…

- **v7.28** (Feb 2026): Vision Teaching Fixes & Single-Leaderboard Site Support
  - **FIXED: Vision teaching callClaude API parameter mismatch** (`run-vision-teaching-50.js`)
    - `callClaude()` expects `{ systemPrompt, userMessage, basePath, imageBase64, domain }` not positional args
    - Returns `{ success, content, usage, error }` not `{ text }`
    - Vision teaching on 50 sites now works: 48% match rate, 41 configs saved
  - **FIXED: Single-leaderboard sites failing with 404** (`orchestrators/scrape-orchestrator.js`)
    - Sites like scrapesgambles.com have one leaderboard visible on `/leaderboard`
    - Crawler detected "stake" from heading "STAKE LEADERBOARD" (method: `detected-name`)
    - No click coordinates available, so orchestrator tried URL `/leaderboard/stake` â†’ 404
    - Old behavior: Skip leaderboard on 404 (lost data)
    - New behavior: Return to original page and extract from there (data already visible)
    - Example: scrapesgambles.com now extracts 3 entries instead of failing
  - **ADDED: Vision teaching scripts**
    - `run-vision-teaching-50.js` - Batch Vision teaching on unique sites
    - `test-learned-configs.js` - Test learned configs with normal scraper
    - `test-with-full-report.js` - HTML report with all entries per leaderboard
    - `run-full-scrape-report.js` - Full scrape using real orchestrator + HTML report
  - **Tested**: scrapesgambles.com âœ… (was failing, now 3 entries)
  - **Full Scrape Test** (30 sites): 29/30 successful (96.7%), 79 leaderboards, 1,018 entries
    - Only failure: crunchyrewards.com (click failed for bsite)
    - Warnings: wager order violations, prize/wager ratio issues, duplicate wagers on some sites

- **v7.27** (Feb 2026): Vision Teaching System - Strategy Validation
  - **FIXED: Vision config not affecting strategy selection** (`core/data-fusion.js`)
    - Problem: Vision taught the scraper about site structure but fusion still picked strategies by confidence alone
    - DOM extraction (90 confidence) could beat markdown (87 confidence) even when markdown was correct
    - Added Vision validation that compares each strategy's rank #1 against Vision's expected values
    - Strategies matching Vision's expected rank #1 wager get +15 confidence boost
    - Strategies not matching get -20 confidence penalty
    - Logged as: `[FUSION] Vision validation: expecting rank #1 wager=$X, prize=$Y`
    - Logged as: `[FUSION] {strategy}: MATCHES/MISMATCH Vision rank #1 - confidence Â±N â†’ {new_confidence}`
  - **ADDED: expectedRank1 config passthrough** (`core/data-extractor.js`, `test-vision-then-scrape.js`)
    - Vision's learned rank #1 data `{ username, wager, prize }` now passed through extraction pipeline
    - Enables fusion layer to validate strategies against Vision's ground truth
  - **Regression tested**: All 4 reference sites pass âœ…

- **v7.26** (Jan 2026): devlrewards.com/packdraw Prize Extraction Fix
  - **FIXED: Prizes for ranks 4-15 showing as $0** (`core/page-scraper.js`)
    - Root cause: Turndown HTMLâ†’Markdown conversion was missing table rules
    - HTML `<table>`, `<tr>`, `<td>` elements weren't being converted to `| cell |` format
    - This caused `parseMarkdownTables()` to find no entries from HTML tables
    - Markdown extraction only found podium (ranks 1-3) and fell back to OCR for rest
    - **Fix**: Added `tableCell` and `tableRow` rules to TurndownService:
      ```javascript
      turndownService.addRule('tableCell', {
        filter: ['th', 'td'],
        replacement: (content) => ' ' + content.trim() + ' |'
      });
      turndownService.addRule('tableRow', {
        filter: 'tr',
        replacement: (content) => '|' + content + '\n'
      });
      ```
    - Also added cleanup rules for styles, scripts, SVGs, and hidden elements
    - Before: `[MD-EXTRACT] tables: found 0 entries`
    - After: `[MD-EXTRACT] tables: found 97 entries` with correct prizes
  - **ADDED: Historical API URL filtering** (`strategies/api-merger.js`)
    - New `URL_PATTERNS.HISTORICAL` array with patterns:
      - `/past-winners/i`, `/previous-leaderboard/i`, `/history/i`
      - `/\byear=\d{4}.*month=/i`, `/\bmonth=\d{1,2}.*year=/i`
      - `/\b20\d{2}-\d{2}\b/i` (date patterns like 2026-01)
    - New `isHistoricalUrl()` function filters these out before API extraction
    - Prevents mixing current leaderboard data with past/archived data
    - Logged as: `[API-MERGE] Filtered 1 historical API response(s)`
  - **ADDED: Debug logging for markdown extraction** (`strategies/markdown-extraction.js`)
    - Automatically saves markdown to `/debug/markdown-{siteName}-{timestamp}.txt`
    - Shows sample entries and prize counts from each parser
    - Enabled for sites containing "devlrewards" or "packdraw" in name
  - **FIXED: siteName not passed to strategies** (`core/data-fusion.js`)
    - Added `input.siteName = siteName` to pass site context to extraction strategies
    - Enables site-specific debugging and handling
  - **Verified**: devlrewards.com/packdraw now extracts 15 entries with correct prizes:
    - Rank 1: $1500, Rank 2: $800, Rank 3: $500
    - Rank 4: $300, Rank 5: $200, Rank 6: $150
    - Ranks 7-10: $75 each, Ranks 11-15: $50 each

- **v7.25** (Jan 2026): wreaths.com HUNT $30,000 Prize Fix
  - **FIXED: wrewards.com HUNT rank #1 missing $30,000 prize** (`strategies/dom-extraction.js`)
    - Problem: Rank #1 showed prize=$700 instead of $30,000
    - Root cause 1: The $30,000 is ONLY in HTML (in a `custom-reward` div), not in API
    - Root cause 2: DOM extraction treated unlabeled amounts as WAGER, not PRIZE
    - Root cause 3: Podium selector order prioritized containers over individual cards
    - **Fix 1**: Added `custom-reward` element detection in `extractFromContainer()`
      - Checks for `[class*="custom-reward"]` elements and extracts their $ value as PRIZE
      - Filters out the custom-reward amount from wager assignment
    - **Fix 2**: Reordered podium selectors to try `[class*="place-1"], [class*="place-2"], [class*="place-3"]` first
      - These match individual podium cards, not the parent container
      - Ensures each card is processed separately
  - **REMOVED: Over-aggressive prize order violation logic** (`core/data-fusion.js`)
    - v7.24 added logic that cleared ALL DOM prizes if ANY ordering "violation" was detected
    - This was too aggressive and lost valid prizes on sites where API and DOM both contribute
    - Example: API had prize1=0, DOM had $30,000 - the logic cleared the $30,000
    - Replaced with comment explaining why it was removed
  - **FIXED: First leaderboard network data preservation** (`orchestrators/scrape-orchestrator.js`)
    - Network data was being cleared before scraping the first/default leaderboard
    - This lost API responses captured during initial page load
    - Now preserves network data for the first leaderboard (21 API responses vs 1 before)
  - **Regression tested**: All 5 reference sites pass âœ…
    - wrewards.com HUNT: #1=$30,000, #2=$1,500, #3=$700 (correct!)

- **v7.24** (Jan 2026): Garbage DOM Prize Detection & Prize Order Validation
  - **FIXED: DOM extracting rank numbers as prizes** (`core/data-fusion.js`)
    - wrewards.com HUNT ranks 32-50 were showing prizes $21-$40 (rank number - 11)
    - Root cause: DOM extraction was picking up rank numbers as prize values
    - Added detection in `fuseEntry()`: reject DOM prizes where prize â‰ˆ rank number
    - Pattern: If prize < 100 AND |prize - rank| <= 15, it's garbage
    - Example: "Rejecting garbage DOM prize: rank 32 prize $21 (likely rank number)"
  - **ADDED: Leaderboard Data Warning System** (`orchestrators/scrape-orchestrator.js`)
    - New `validateLeaderboardData()` function detects suspicious patterns:
      - Prize order violations (higher rank users have lower prizes)
      - Garbage prizes (prize â‰ˆ rank number for ranks > 20)
      - Wager order violations (lower rank users have higher wagers)
      - Absurd prize/wager ratios (prize > wager for many entries)
      - All zeros (wager or prize all 0)
      - Duplicate wagers (Frankenstein entries from DOM extraction)
    - Warnings added to output: `result.warnings = [{ leaderboard, issues }]`
    - Logged as: `[WARNING] {leaderboard}: {issues}`

- **v7.23** (Jan 2026): Smart Amount Assignment for Two-Amount Entries
  - **FIXED: paxgambles.com DICEBLOX rows 4-5 had wager/prize swapped** (`shared/extraction.js`)
    - Root cause: Site shows TWO amounts per list entry: "$ 20" (prize) then "577,399.80" (wager)
    - First fix attempt (currency = prize) broke ACEBET which has single amount with $
    - **Final solution: Two-phase amount assignment**
      1. Collect all amounts found in entry with `{ amount, hasCurrency }` metadata
      2. Apply smart assignment based on count and currency pattern:
         - **ONE amount**: It's the WAGER (determines leaderboard rank position), prize = $0
         - **TWO amounts**: Check currency pattern:
           - If first has $ and second doesn't â†’ first=prize, second=wager (paxgambles pattern)
           - Otherwise â†’ first=wager, second=prize (default)
    - Example: foxzaayy now correctly shows wager=$577,399.80, prize=$20 (was backwards)
  - **Code location**: `shared/extraction.js` lines ~1292-1331 (list entry parser)
  - **Regression tested**: paxgambles.com âœ…, goatgambles.com âœ…, wrewards.com âœ…, yeeterboards.com âœ…
  - **Pending**: devlrewards.com has page loading/timing issue (separate fix needed)

- **v7.22** (Jan 2026): CRITICAL REGRESSION FIX - Reverted v7.19-v7.21 Breaking Changes
  - **REVERTED: Wager/Prize Swap Logic** (`shared/extraction.js`)
    - v7.20 added logic that swapped wager/prize when second amount > first
    - This BROKE paxgambles.com (wager=$20, prize=$577K shown backwards)
    - REMOVED the swap logic entirely - sites have different orderings
    - Now: first unlabeled amount = wager, second = prize, NO SWAPPING
  - **FIXED: Deduplication Key** (`shared/extraction.js`)
    - Changed from `username|wager` to `username|rank`
    - Old key failed when different parsers extracted different wager values
    - Same user at same rank = duplicate (regardless of wager value)
    - Fixes duplicate entries on paxgambles.com (was showing 11 instead of 10)
  - **FIXED: devlrewards.com 52 entries â†’ ~20 entries**
    - Root cause was broken dedup allowing duplicates through
    - New rank-based dedup properly catches podium + list overlap
  - **ADDED: CRITICAL WARNING section at top of this document**
  - **ADDED: Critical Rule #7 - NEVER BREAK WORKING SITES**
  - **ADDED: Mandatory regression test protocol**
  - **Sites Fixed**: paxgambles.com, devlrewards.com, wrewards.com (all previously broken by v7.19-v7.21)

- **v7.21** (Jan 2026): Rank Number vs Prize Disambiguation & Email Username Fix âš ï¸ CAUSED REGRESSIONS
  - **FIXED: Rank Numbers Being Interpreted as Prizes** (`shared/extraction.js`)
    - On ROOBET ranks 16-20, the next rank number (e.g., `17`) was being parsed as a prize amount
    - Added logic to detect "likely rank numbers": bare integers 1-100 without currency symbol or decimal
    - Pattern check: `amount <= 100 && !hasCurrency && !hasDecimal && Number.isInteger(amount)`
    - Applied fix to both podium parser (Method 1) and list parser
    - Example: ROOBET rank 16 now shows prize=$0 instead of incorrect prize=$17
  - **FIXED: Email Usernames Rejected as Website Names** (`shared/entry-validation.js`)
    - Usernames ending in `.com` were rejected by `looksLikeWebsiteName()` function
    - But email addresses like `***********5@gmail.com` are valid usernames, not websites
    - Added check: if username contains `@`, skip the domain pattern validation
    - Example: RAIN rank 7 `***********5@gmail.com` now captured instead of being filtered out
  - **Sites Fixed**: goatgambles.com ROOBET (prizes now correct), RAIN (all 10 entries captured)

- **v7.20** (Jan 2026): goatgambles.com Complete Fix - Markdown Image Handling & Prize Extraction
  - **FIXED: Currency Icons Breaking Amount Parsing** (`shared/extraction.js`)
    - Sites like goatgambles.com have `![Currency](/assets/clash-coin.svg)` before amounts
    - Added `stripImages()` helper to remove markdown image syntax before pattern matching
    - Updated `parseMarkdownAmount()` to strip `![alt](url)` patterns from text
    - Updated both podium and list parsers to use `stripImages()` before testing amount patterns
    - Example: `![Currency](...) 1,216.16` now correctly parses as `$1,216.16`
  - **FIXED: Labeled Prize Format Not Recognized** (`shared/extraction.js`)
    - goatgambles list entries use `Prize: 120.00` format (labeled, not bare amount)
    - Added `labeledPrizePattern` to both Method 1 (inline Wagered:) and Method 2 (separate Wagered:)
    - Pattern: `/^Prize:\s*(?:!\[[^\]]*\]\([^)]*\)\s*)?[$â‚¬Â£...]?\s*([\d,]+(?:\.\d+)?)/i`
    - Now correctly extracts prizes for all list entries (rank 4+)
    - Example: RAIN rank 4 now shows prize=$120 instead of prize=$0
  - **FIXED: Short Censored Usernames Being Rejected** (`shared/extraction.js`)
    - Username `**u` (2 censored chars + 'u') was being rejected after stripping leading `**`
    - Updated `cleanMarkdownUsername()` to preserve leading asterisks if stripping would leave <2 chars
    - Example: RAIN rank 5 `**u` now correctly captured instead of being dropped
  - **Sites Fixed**: goatgambles.com (RAIN, ROOBET, CLASH, CSGOWIN, CSGOGEM - all leaderboards now extract correctly)

- **v7.19** (Jan 2026): UI Text Filtering & Smart Deduplication in Markdown Extraction
  - **FIXED: Timer UI Text Captured as Usernames** (`shared/utils.js`, `shared/extraction.js`)
    - "HRS", "MINS", "SECS" timer abbreviations were being extracted as usernames
    - Added "hrs", "mins", "secs" to `UI_SINGLE_WORDS` blacklist in utils.js
    - Added `isUIText()` validation in `cleanMarkdownUsername()` to filter UI garbage
    - Now rejects timer labels before they become entry candidates
    - Example: goatgambles.com had "SECS" at rank 12 - now correctly filtered
  - **FIXED: "Wagered: X.XX" Labels Captured as Usernames** (`shared/extraction.js`)
    - Markdown list parser was misidentifying "Wagered: 532.51" as a username
    - Added pattern filter in `cleanMarkdownUsername()` for label:value patterns
    - Pattern: `/^(wagered|wager|prize|reward|bonus):\s*[\d,.]+$/i`
    - Returns `null` for these patterns so they're not included as entries
    - Example: goatgambles.com rain leaderboard had "Wagered: 532.51" as rank 5
  - **FIXED: Duplicate Entries with Same Username but Different Wager** (`strategies/markdown-extraction.js`)
    - Same user was appearing twice: once from podium (with wager) and once from list (0 wager)
    - Old dedup used `username|wager` key, so different wager values created "duplicates"
    - New smart deduplication groups by username first, then merges intelligently:
      - If only one entry has wager > 0, use that entry (authoritative)
      - If multiple entries have same username, prefer the one with actual wager data
      - For truly different users (e.g., "Anonymous"), use rounded wager to catch parsing differences
    - Example: goatgambles.com/roobet went from 40 entries (20 duplicates) to 20 clean entries

- **v7.18** (Jan 2026): Historical URL Filtering, Zero-Wager & Hidden Username Support
  - **FIXED: Scraper Navigating to Historical/Previous Leaderboard URLs** (`shared/page-navigation.js`)
    - `navigateToLeaderboardSection()` was finding and clicking "Previous Leaderboard" links
    - Added early-exit check: if already on a `/leaderboard` URL (not `/prev-leaderboard`), skip navigation
    - Added historical URL filtering in navigation link discovery (Strategy 2)
    - Filters both URL patterns (`prev-leaderboard`, `past-leaderboard`, `archive`) and link text (`previous leaderboard`)
    - Example: paxgambles.com was scraping `/prev-leaderboard/diceblox` instead of `/leaderboard/diceblox`
    - Logged as `[NAV] Already on leaderboard URL: {url}, no navigation needed`
  - **FIXED: Users with $0 Wager Being Excluded** (`shared/extraction.js`)
    - `parseMarkdownPodium()` and `parseMarkdownList()` required `wager > 0` to include entries
    - Changed to `wager >= 0` to include users who are on leaderboard but haven't wagered yet
    - Example: paxgambles.com/cases rank 10 user `Mu*****` with $0 wagered was being dropped
  - **FIXED: Podium Ranking by Prize Amount** (`strategies/markdown-extraction.js`)
    - Center-podium layouts show #2 | #1 | #3 but HTML/markdown parses left-to-right
    - Now sorts podium entries by PRIZE amount (highest prize = rank 1)
    - Falls back to wager amount as secondary sort if prizes are equal/missing
    - This is "common sense" - the player with highest prize IS rank 1
    - Example: paxgambles.com/cases - rank 1 has $150 prize (was wrongly assigned to rank 2 in parsing order)
    - Logged as `[MD-EXTRACT] Sorted podium by prize: {username}=$${prize}, ...`
  - **ADDED: Hidden Username Placeholder Support** (`shared/extraction.js`, `shared/utils.js`)
    - Some sites display only avatars with no username text (e.g., empty quotes `""`)
    - `cleanMarkdownUsername()` now converts empty/quote-only usernames to `[hidden]` placeholder
    - `validateUsername()` now accepts `[hidden]` as valid (60% confidence, reason: `hidden_username_placeholder`)
    - Ensures ALL leaderboard entries are captured even when username is not displayed
    - Example: paxgambles.com/cases rank 4 user with Steam avatar but no visible username
  - **FIXED: Substring Keywords Causing Duplicate Clicks** (`shared/site-detection.js`)
    - `findSiteSwitchers()` was adding both "rainbet" and "rain" as separate switchers
    - Both keywords matched the same element (img alt="Rainbet Wager" contains both)
    - Added deduplication: removes keywords that are substrings of longer keywords
    - Example: devlrewards.com had "rainbet", "rain", "packdraw" â†’ now "rainbet", "packdraw"
    - Logged as `[CLICK] Filtering out {keyword} with invalid coords: substring_of_longer_keyword`
  - **FIXED: "USERS" Being Captured as Username** (`shared/utils.js`)
    - Added "users" (plural) to `UI_SINGLE_WORDS` blacklist - "user" was already there
    - Example: goatgambles.com was including "USERS" header as a leaderboard entry at rank 9
    - Now correctly rejected with reason `ui_text`

- **v7.17** (Jan 2026): Podium Ranking & Wager/Prize Assignment Fixes
  - **FIXED: Podium Entries Ranked by Text Order Instead of Wager** (`strategies/markdown-extraction.js`)
    - Podium layouts visually display as #2 | #1 | #3 (center is rank 1) but HTML/markdown is parsed left-to-right
    - Added `_rankFromImage` flag in `extraction.js` to track if rank came from image path
    - When no image-detected ranks, now sorts podium entries by wager descending (highest = rank 1)
    - Example: paxgambles.com Magqing with $1,988,902 was incorrectly rank 2, now correctly rank 1
    - Logged as `[MD-EXTRACT] Ranking podium entries by wager (highest = rank 1)`
  - **FIXED: Wager/Prize Values Swapped in Header Entries** (`shared/extraction.js`)
    - `parseMarkdownHeaderEntries()` was assigning first `$XXX` amount as prize instead of wager
    - Root cause: `prizePattern` matched before checking for "Wagered" label context
    - Added tracking for "Wagered" label - when seen, next amount is assigned as wager
    - Example: scrapesgambles.com syn** had wager/prize swapped, now correct ($164,005 wager, $2,500 prize)
  - **FIXED: URL Navigation Fallback Going to Homepage** (`orchestrators/scrape-orchestrator.js`)
    - When URL pattern navigation failed, scraper was returning to homepage instead of leaderboard page
    - Now saves `urlBeforeNavAttempt` and navigates back to it instead of `leaderboard.url`
    - Prevents data loss when site doesn't support `/leaderboard/{keyword}` URL patterns
  - **FIXED: Cyrillic Characters and Bracket Suffixes in Usernames** (`shared/utils.js`)
    - Extended `isValidUsername()` to accept Cyrillic characters (U+0400-U+04FF range)
    - Added support for bracket suffixes like `[VIP]`, `(PRO)` in usernames
    - Fixes Russian/Ukrainian usernames being rejected as invalid

- **v7.16** (Jan 2026): API Entry Preservation & Short Censored Username Fix
  - **FIXED: API Entries Being Filtered Incorrectly** (`core/data-fusion.js`)
    - Single-source API entries were being filtered if they exceeded the DOM's max rank
    - Now trusts API data (structured source) even without cross-validation
    - Only filters DOM/OCR entries beyond the effective max rank
    - Example: jonkennleaderboard.com/chicken now extracts 15 entries (was 12)
    - Example: jonkennleaderboard.com/csbattle now extracts 10 entries (was 7)
  - **FIXED: Short Censored Usernames Being Rejected** (`shared/utils.js`)
    - Username `A*` (heavily censored) was rejected for "insufficient_letters"
    - Extended censored username detection to include 1 asterisk if length <= 4
    - Single-letter + asterisk usernames are now valid (e.g., `A*`, `J*`)
    - Example: csbattle rank 9 `A*` is now captured

- **v7.15** (Jan 2026): Dropdown Menu Detection & Profile-Known Leaderboards
  - **ADDED: Enhanced Dropdown Detection** (`shared/site-detection.js`)
    - Multi-strategy approach to open dropdown menus containing leaderboard links
    - Clicks dropdown buttons (`.dropdown-btn`, `.dropdown-toggle`, etc.)
    - Force-shows hidden dropdown content via CSS manipulation
    - Handles `/leaderboard/{keyword}` URL pattern extraction for nested paths
    - Relaxed bounding box constraints for dropdown items (may be off-screen initially)
  - **ADDED: Initial Domain Navigation** (`shared/page-navigation.js`)
    - `navigateToLeaderboardSection()` now navigates to baseUrl first if page is `about:blank`
    - Prevents discovery from running on empty page when browser launches fresh
  - **ADDED: Profile-Known Leaderboards Fallback** (`orchestrators/scrape-orchestrator.js`)
    - Merges previously-known leaderboards from site profile with newly discovered ones
    - Handles cases where dropdown detection fails or leaderboards are hidden
    - Uses `method: 'profile-known'` for URL-based navigation to stored leaderboard URLs
    - Example: jonkennleaderboard.com dropdown items now scraped via profile even if not detected

- **v7.14** (Jan 2026): Duplicate Rank Fusion Fix
  - **FIXED: Same Rank Appearing Multiple Times from Different Sources** (`core/cross-validator.js`)
    - `buildEntryMap()` was using `rank|wager` as key, causing same rank with different wagers to create duplicates
    - Example: jonkennleaderboard.com/chicken had rank 1 twice: once with wagerAmount, once with XP value
    - Changed key to use `rank` only (wager is now resolved during fusion, not used for matching)
    - Same user at same rank now correctly fuses into single entry, picking best wager from highest-confidence source
    - Fixes sites where API returns wagerAmount but DOM displays XP/deposits/other fields
    - Before: chicken leaderboard had 27 entries with duplicate ranks 1-15
    - After: chicken leaderboard has 12 unique entries with correct wager values from API

- **v7.13** (Jan 2026): Escaped Asterisk Username Length Fix
  - **FIXED: Usernames with Many Censored Asterisks Being Skipped** (`shared/extraction.js`)
    - `parseMarkdownList()` was rejecting usernames that exceeded 50 characters
    - Markdown escapes asterisks as `\*`, so `Ad************************ys` (28 chars) becomes 52 chars
    - Now checks length AFTER removing backslash escapes: `nextLine.replace(/\\/g, '').length`
    - Fixes betjuicy.com csgoroll rank 9 (`Ad************************ys`) not being captured
    - Logged as: Now extracts all 10 ranks instead of 9

- **v7.12** (Jan 2026): Podium Extraction & Stale API Detection Fixes
  - **FIXED: Podium Not Capturing Ranks 1-3** (`shared/extraction.js`)
    - Added support for markdown header format podiums (`### Wagered`, `### $amount`)
    - Added rank detection from image paths (e.g., `rank1-hex.svg`, `rank2-hex.svg`)
    - Added separator line skipping (`---`, `===` markdown underlines)
    - Added duplicate entry deduplication for mobile/desktop views of same podium
    - Sites like betjuicy.com now correctly extract all 3 podium entries
  - **FIXED: Detected Ranks Being Overwritten** (`strategies/markdown-extraction.js`)
    - Previously, ranks detected from image paths were overwritten by prize-based sorting
    - Now preserves image-detected ranks (1, 2, 3) when available
    - Falls back to prize/wager sorting only when no image-based ranks detected
    - Logged as `[MD-EXTRACT] Using detected ranks from images: {ranks}`
  - **FIXED: Stale API Data Corrupting Results** (`core/data-fusion.js`)
    - Added detection for stale/cached API data that doesn't match current leaderboard
    - Compares API wagers against markdown wagers (exact match + 1% tolerance)
    - Removes stale API data from fusion when no wager matches found
    - Fixes issue where same API data appeared across all leaderboards on sites like betjuicy.com
    - Logged as `[FUSION] Stale API data detected: no wager matches between API and markdown`
  - **ADDED: Batch Runner URL Arguments** (`orchestrators/batch-runner.js`)
    - Batch runner now accepts URL arguments for testing specific sites
    - Example: `node batch-runner.js https://site1.com https://site2.com`
    - Skips loading from websites.txt when URLs provided on command line

- **v7.11** (Jan 2026): Duplicate Detection & URL-Specific Leaderboard Fixes
  - **FIXED: Duplicate Leaderboard Names** (`shared/site-detection.js`)
    - Added deduplication logic to `detectAllSiteNames()`
    - Filters out keywords that are substrings of other longer keywords
    - Example: if both "stake" and "rostake" are detected, keeps only "rostake"
    - Prevents duplicate leaderboard entries from the same URL
    - Logged as `[DETECT] Filtering out "{keyword}" - it's a substring of another detected keyword`
  - **FIXED: Podium Ranking Order** (`strategies/markdown-extraction.js`)
    - Podium entries now sort by wager descending when no prizes are extracted
    - Previously used visual left-to-right order which was incorrect (many sites show #2 | #1 | #3)
    - Highest wager now correctly gets rank 1 instead of relying on visual position
    - Logged as `[MD-EXTRACT] Podium has no prizes - ranking by wager instead`
  - **FIXED: URL-Specific Leaderboard Detection** (`orchestrators/scrape-orchestrator.js`)
    - URLs containing specific keywords (e.g., `/leaderboard/csgogem`) now skip discovery
    - Previously, scraping `/leaderboard/csgogem` would discover and scrape ALL leaderboards
    - Now only scrapes the leaderboard specified in the URL path
    - Pattern detected: `/leaderboard(s)/{keyword}` where keyword matches `keywords.txt`
    - Logged as `[ORCHESTRATE] URL contains specific keyword: {keyword}`

- **v7.10** (Jan 2026): Entry Fusion & Username Validation Fixes
  - **FIXED: Username Masking Mismatch in Fusion** (`core/cross-validator.js`)
    - Changed `buildEntryMap` to use `rank|wager` as primary key instead of `username|wager`
    - Fixes issue where same user with different masking across sources (e.g., `0**i***` vs `**i***`) created duplicate entries
    - Falls back to `username|wager` for entries without rank
  - **FIXED: Out-of-Range Single-Source Entries** (`core/data-fusion.js`)
    - Added filter to reject single-source entries with ranks beyond the max verified rank
    - Catches DOM extraction picking up elements outside the actual leaderboard (e.g., rank 61 when API has 50)
    - Logged as `[FUSION] Filtering out-of-range entry: {username} (rank {rank}) - beyond max verified rank {max}`
  - **FIXED: Username Starting with Numbers Being Stripped** (`shared/extraction.js`)
    - Changed `cleanMarkdownUsername` regex from `^#?\d+\s*` to `^#\d+\s*`
    - Prevents usernames like `24/7 maluma maluma` from having the `24` stripped as a rank prefix
    - Only strips digits when preceded by `#` or followed by clear separators (`. ) : space`)
  - **FIXED: 3-Word Usernames Rejected as Invalid** (`shared/utils.js`)
    - Changed `wordCount <= 2` to `wordCount <= 3` in `unicode_pattern` and `fallback_accepted` checks
    - Consistent with existing `wordCount > 3` rejection rule
    - Fixes usernames like `24/7 maluma maluma` being rejected as `pattern_mismatch`

- **v7.9** (Jan 2026): Duplicate Entry Filtering & Data Quality
  - **FIXED: DOM Extraction Duplicate Entries** (`core/data-fusion.js`)
    - Added filter to reject single-source entries with duplicate wager values
    - Catches DOM extraction bugs where data gets mixed between users (e.g., username from #1, wager from #3, rank #10)
    - Multi-source entries are kept regardless of duplicate wagers (verified by cross-validation)
    - Logged as `[FUSION] Filtering suspicious entry: {username} (rank {rank}) - duplicate wager`
  - **Root Cause**: The DOM extractor was merging podium cards with list entries incorrectly, creating Frankenstein entries that mixed fields from different users
  - **ADDED: Admin Database Viewer** (`admin-viewer.js`)
    - Temporary HTML page for viewing database contents at `http://localhost:3333`
    - Shows sites, leaderboards, snapshots, and entries organized by domain
    - Run with `node admin-viewer.js`

- **v7.8** (Jan 2026): Production Infrastructure & Database Auto-Save
  - **ADDED: Production Server Infrastructure**
    - Scraper Server (Hetzner CCX33): 8 vCPU, 32GB RAM @ 159.69.35.141
      - Location: `/var/www/argus`
      - Node.js 20, Playwright, PM2 installed
    - Database Server (Hetzner CPX42): 8 vCPU, 16GB RAM @ 46.224.236.76
      - PostgreSQL 17 + TimescaleDB 2.24.0
      - Database name: `argus`
      - Firewall: SSH only, PostgreSQL restricted to scraper server IP
  - **ADDED: Database Auto-Save** (`lbscraper/shared/db-save.js`)
    - `saveToDatabase(domain, result)` - Auto-saves scrape results to database
    - Auto-creates `LeaderboardSite` for new domains
    - Creates `LeaderboardCycle` per site/leaderboard combo
    - Creates `LeaderboardSnapshot` with confidence, method, totals
    - Bulk inserts `LeaderboardEntry` records
    - Updates `site.lastScrapedAt` on success
  - **MODIFIED: Scraper Integration** (`lbscraper/new-run-scraper.js`)
    - Added database save after JSON save (wrapped in try/catch)
    - JSON save still works if database save fails
  - **Connection String Format**:
    ```
    DATABASE_URL="postgresql://postgres:PASSWORD@46.224.236.76:5432/argus"
    ```

- **v7.7** (Jan 2026): TimescaleDB Integration & JSON Debug Logging
  - **ADDED: TimescaleDB Hypertables** (`prisma/migrations/20260126_add_timescaledb_and_json_logging/`)
    - `leaderboard_snapshots` and `leaderboard_entries` converted to hypertables
    - 7-day chunk partitioning for efficient time-range queries
    - Indexes optimized for username + time queries
    - New `scrapedAt` field on `LeaderboardEntry` for partitioning
  - **ADDED: JSON Debug Logging** (`lbscraper/shared/json-logger.js`)
    - Timestamped JSON files saved to `results/logs/YYYY-MM-DD/`
    - Configurable retention period (default 48 hours, max 720 hours)
    - Optional auto-cleanup (can be disabled to keep logs indefinitely)
    - Storage stats tracking (total files, size, oldest/newest, per-date breakdown)
  - **ADDED: Logging Configuration** (`scraper_configs` table)
    - `JSON_LOGGING_ENABLED` - Enable/disable logging (default: true)
    - `JSON_AUTO_CLEANUP_ENABLED` - Enable/disable auto-deletion (default: true)
    - `JSON_RETENTION_HOURS` - Hours to keep logs (default: 48, max: 720)
    - ENV fallback: `SCRAPER_JSON_LOGGING`, `SCRAPER_JSON_AUTO_CLEANUP`, `SCRAPER_JSON_RETENTION_HOURS`
  - **ADDED: Admin API Endpoints** (`src/modules/scraper/scraper.controller.ts`)
    - `GET /admin/scraper/config/json-logging` - Get config + storage stats
    - `PUT /admin/scraper/config/json-logging` - Update all logging options
    - `POST /admin/scraper/logs/cleanup` - Manual cleanup (uses current retentionHours)
    - `GET /admin/scraper/logs/stats` - Storage stats only
  - **MODIFIED: Config Loader** (`lbscraper/shared/config.js`)
    - Added `getJsonLoggingConfig()`, `isJsonLoggingEnabled()`, `isAutoCleanupEnabled()`, `getRetentionHours()`
    - Database priority with ENV fallback pattern
  - **MODIFIED: Scraper Integration** (`lbscraper/new-run-scraper.js`)
    - Integrated JSON logging after saving to `results/current/`
    - Probabilistic auto-cleanup (10% chance per run to avoid overhead)

- **v7.6** (Jan 2025): Prize Validation & Fusion Deduplication Fixes
  - **FIXED: Prize Validation Beyond Cutoff** (`orchestrators/scrape-orchestrator.js`)
    - Entries beyond the prize table cutoff now correctly have `prize: 0`
    - Previously, geometric extraction could incorrectly assign rank number as prize (e.g., rank 21 had `prize: 21`)
    - Prize lookup map validates prizes against the extracted prize table
    - Only ranks with explicit prize data in the table receive non-zero prizes
  - **FIXED: totalPrizePool Calculation Fallback** (`orchestrators/scrape-orchestrator.js`)
    - If no API metadata provides `_totalPrizePool`, now sums prizes from extracted entries
    - Works for sites without prize metadata endpoints (e.g., elliotrewards.gg)
    - Teacher Mode results also calculate `totalPrizePool` from entry prizes
  - **FIXED: Markdown List Parser Missing Rank 3** (`shared/extraction.js`)
    - Removed restriction that skipped ranks 1-3 in `parseMarkdownList()`
    - Some sites (e.g., elliotrewards.gg) display rank 3 in list format, not podium
    - Deduplication handles overlap if podium parser also finds ranks 1-3
  - **FIXED: Same Username Different Wagers in Fusion** (`core/cross-validator.js`, `strategies/markdown-extraction.js`)
    - Changed `buildEntryMap()` key from username to `username|wager`
    - Masked usernames like "El*******" can now appear in multiple ranks with different wagers
    - Previously rank 2 was lost when same username appeared in ranks 2 and 3
  - **FIXED: Low Confidence Strategy Filtering** (`core/data-fusion.js`)
    - Strategies below `minConfidence` (default 50) are now excluded from fusion
    - Prevents garbage OCR entries from corrupting fused results
  - **Verified Working**: elliotrewards.gg shuffle ($5,000 pool, ranks 1-10 with correct prizes)

- **v7.5** (Jan 2025): Output Format Standardization
  - **CHANGED: Entry ID Structure** (`orchestrators/scrape-orchestrator.js`)
    - Removed per-entry `id` fields - entries now only have `extractedAt`
    - ONE `id` per leaderboard extraction (was already correct, entries were wrong)
    - Teacher Mode results updated with same structure
  - **ADDED: Leaderboard Totals**
    - `totalPrizePool`: Extracted from API metadata, now surfaced in output
    - `totalWagered`: Sum of all entry wagers, calculated at extraction time
  - **FIXED: Prize Propagation** (`strategies/api-merger.js`, `strategies/api-extraction.js`, `core/data-fusion.js`)
    - `_totalPrizePool` now preserved through extraction pipeline
    - Fixed `extractPrizeTableFromResponses()` to preserve metadata
    - Fixed `extractPrizeTable()` to check `_mergedPrizes` first
    - Fixed `fusePrizes()` to preserve `_totalPrizePool` from best source
  - **FIXED: additionalPrizes Parsing** (`strategies/api-merger.js`)
    - Added `parseInt()` to rank extraction from `prizeNumber` field
    - Changed `prize > 0` to `prize >= 0` to allow $0 prizes
  - **Verified Working**: GAMDOM ($100K pool), PACKDRAW ($50K), LOOTBOX ($25K)
  - **Note**: wrewards.com "hunt" is actually "Gpoints" (their internal points leaderboard). The API returns `casinoProvider=WREWARDS` for this. No separate leaderboard-info API is called for Gpoints, only for external casino partners.

- **v7.4** (Jan 2025): Prize Data Extraction from Split APIs
  - **FIXED: Numbered Prize Field Detection** (`strategies/api-merger.js`)
    - Added detection for `prize1`, `prize2`, `prize3` numbered field patterns
    - Added `additionalPrizes` array extraction (wrewards.com format: `{prizeNumber, amount}`)
    - Added `totalPrizePool` field extraction
    - New `isLeaderboardMetadata()` function detects metadata response structure
  - **FIXED: URL Pattern Matching** (`strategies/api-merger.js`)
    - Fixed `/rewards/i` pattern incorrectly matching site names like "WREWARDS"
    - Changed to `/\/rewards\b(?!\.)/i` to only match URL paths
    - Added explicit wrewards.com endpoint patterns:
      - `/leaderboard-info\?/i` for metadata responses
      - `/list-winner/i` for past winners
      - `/\bld-leaders\b/i` for user entries
      - `/leaders\?.*viewState/i` for expanded view
  - **FIXED: Detection Priority Order** (`strategies/api-merger.js`)
    - User arrays now checked BEFORE metadata in `analyzeDataContent()`
    - Prevents user responses from being miscategorized as "prizes"
  - **UPDATED: additionalPrizes Format Support** (`shared/api-patterns.js`)
    - Supports field variations: `prizeNumber`, `prize_number`, `position`, `rank`
    - Supports value variations: `amount`, `value`, `prize`, `reward`
  - **Verified Working**: GAMDOM, PACKDRAW, LOOTBOX (positions 1-15), HUNT (1-10)

- **v7.3** (Jan 2025): Hybrid Extraction & Cross-Validation Architecture
  - **NEW: Data Fusion Layer** (`core/data-fusion.js`)
    - Runs ALL viable extraction strategies, not just first success
    - Fuses results from multiple sources
    - Tags entries with verification status (verified, single_source, disputed)
  - **NEW: Cross-Validator** (`core/cross-validator.js`)
    - Compares extraction results between strategies
    - Calculates agreement scores
    - Detects discrepancies and field-level conflicts
    - Confidence boost/penalty based on source agreement
  - **NEW: API Merger** (`strategies/api-merger.js`)
    - Handles sites with split APIs (users in one call, prizes in another)
    - Categorizes responses by content type
    - Merges complementary data
  - **NEW: Quality Scorer** (`core/quality-scorer.js`)
    - Multi-dimensional quality scoring
    - Entry completeness, source agreement, data validity
    - Historical consistency, learned pattern match
  - **UPDATED: Data Extractor**
    - Now uses fusion layer by default (useFusion: true)
    - Falls back to legacy sequential extraction if fusion fails
    - Passes learned patterns to strategies
  - **UPDATED: Learning System**
    - Records successful extraction patterns
    - Stores field mappings and preferred sources
    - Applies learned patterns on subsequent runs

- **v7.2** (Jan 2025): Click mechanism and network capture improvements
  - Added image-based click strategy for logo buttons (sites like wrewards.com)
  - Fixed network data clearing timing (clear BEFORE switcher click, not after)
  - Improved fingerprint detection to skip featured users (collect 10 usernames, skip first 3)
  - Added more wager field variations (points, gpoints, balance, score, coins)
  - Relaxed quality check for sites without wager data in API
  - Added periodic re-discovery documentation (24-hour interval)
  - Added non-standard URL patterns documentation

- **v7.1** (Jan 2025): Extraction and tracking enhancements
  - Added `markdown-extraction.js` strategy (priority 1.5)
  - Added unique UUIDs and timestamps to all outputs
  - Added multi-currency parsing support
  - Added Teacher Mode fallback for failed extractions
  - Fixed false positive keyword detection (word boundary matching)
  - Added `SCRAPER_HEADLESS` environment variable

- **v7.0** (Jan 2025): Modular architecture refactor
  - Created 3 pillars: crawler, scraper, extractor
  - Extracted 4 strategies: api, dom, geometric, ocr
  - Created orchestrators: scrape-orchestrator, batch-runner
  - Added Admin API in NestJS
  - All files under 600 lines

- **v6.0**: Legacy monolith (current-scraper.js, 3,638 lines)
  - Being deprecated in favor of modular architecture

---

*Last updated: February 3, 2026 (v7.29)*
*Maintainer: AI assistants should update this document when making architectural changes*
